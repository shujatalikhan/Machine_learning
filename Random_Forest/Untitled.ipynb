{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification using tf.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tfimage_gen = ImageDataGenerator(rescale=1./255, horizontal_flip=True)\n",
    "\n",
    "train_data_gen = image_gen.flow_from_directory(\n",
    "                                                batch_size=batch_size, \n",
    "                                                directory=train_dir, \n",
    "                                                shuffle=True, \n",
    "                                                target_size=(IMG_SHAPE,IMG_SHAPE)\n",
    "                                                )\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Conv2D,Flatten,Dropout,MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to build our image classifier, we can begin by downloading the flowers dataset. We first need to download the archive version of the dataset and after the download we are storing it to \"/tmp/\" directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After downloading the dataset, we need to extract its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_URL = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "\n",
    "zip_file = tf.keras.utils.get_file(origin=_URL, \n",
    "                                   fname=\"flower_photos.tgz\", \n",
    "                                   extract=True)\n",
    "\n",
    "base_dir = os.path.join(os.path.dirname(zip_file), 'flower_photos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The  dataset we downloaded contains images of 5 types of flowers:\\n\\n1. Rose\\n2. Daisy\\n3. Dandelion\\n4. Sunflowers\\n5. Tulips\\n\\nSo, let's create the labels for these 5 classes: \""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''The  dataset we downloaded contains images of 5 types of flowers:\n",
    "\n",
    "1. Rose\n",
    "2. Daisy\n",
    "3. Dandelion\n",
    "4. Sunflowers\n",
    "5. Tulips\n",
    "\n",
    "So, let's create the labels for these 5 classes: '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=[\"roses\",\"daisy\",\"dandelion\",\"sunflowers\",\"tulips\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Also, The dataset we have downloaded has following directory structure.\\n\\nflower_photos\\n|__ diasy\\n|__ dandelion\\n|__ roses\\n|__ sunflowers\\n|__ tulips\\nAs you can see there are no folders containing training and validation data. Therefore, we will have to create our own training and validation set. Let's write some code that will do this.\\n\\nThe code below creates a train and a val folder each containing 5 folders (one for each type of flower). It then moves the images from the original folders to these new folders such that 80% of the images go to the training set and 20% of the images go into the validation set. In the end our directory will have the following structure:\\n\\nflower_photos\\n|__ diasy\\n|__ dandelion\\n|__ roses\\n|__ sunflowers\\n|__ tulips\\n|__ train\\n    |______ daisy: [1.jpg, 2.jpg, 3.jpg ....]\\n    |______ dandelion: [1.jpg, 2.jpg, 3.jpg ....]\\n    |______ roses: [1.jpg, 2.jpg, 3.jpg ....]\\n    |______ sunflowers: [1.jpg, 2.jpg, 3.jpg ....]\\n    |______ tulips: [1.jpg, 2.jpg, 3.jpg ....]\\n |__ val\\n    |______ daisy: [507.jpg, 508.jpg, 509.jpg ....]\\n    |______ dandelion: [719.jpg, 720.jpg, 721.jpg ....]\\n    |______ roses: [514.jpg, 515.jpg, 516.jpg ....]\\n    |______ sunflowers: [560.jpg, 561.jpg, 562.jpg .....]\\n    |______ tulips: [640.jpg, 641.jpg, 642.jpg ....]\\n    \""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Also, The dataset we have downloaded has following directory structure.\n",
    "\n",
    "flower_photos\n",
    "|__ diasy\n",
    "|__ dandelion\n",
    "|__ roses\n",
    "|__ sunflowers\n",
    "|__ tulips\n",
    "As you can see there are no folders containing training and validation data. Therefore, we will have to create our own training and validation set. Let's write some code that will do this.\n",
    "\n",
    "The code below creates a train and a val folder each containing 5 folders (one for each type of flower). It then moves the images from the original folders to these new folders such that 80% of the images go to the training set and 20% of the images go into the validation set. In the end our directory will have the following structure:\n",
    "\n",
    "flower_photos\n",
    "|__ diasy\n",
    "|__ dandelion\n",
    "|__ roses\n",
    "|__ sunflowers\n",
    "|__ tulips\n",
    "|__ train\n",
    "    |______ daisy: [1.jpg, 2.jpg, 3.jpg ....]\n",
    "    |______ dandelion: [1.jpg, 2.jpg, 3.jpg ....]\n",
    "    |______ roses: [1.jpg, 2.jpg, 3.jpg ....]\n",
    "    |______ sunflowers: [1.jpg, 2.jpg, 3.jpg ....]\n",
    "    |______ tulips: [1.jpg, 2.jpg, 3.jpg ....]\n",
    " |__ valimage_gen = ImageDataGenerator(rescale=1./255, horizontal_flip=True)\n",
    "\n",
    "train_data_gen = image_gen.flow_from_directory(\n",
    "                                                batch_size=batch_size, \n",
    "                                                directory=train_dir, \n",
    "                                                shuffle=True, \n",
    "                                                target_size=(IMG_SHAPE,IMG_SHAPE)\n",
    "                                                )\n",
    "    |______ daisy: [507.jpg, 508.jpg, 509.jpg ....]\n",
    "    |______ dandelion: [719.jpg, 720.jpg, 721.jpg ....]\n",
    "    |______ roses: [514.jpg, 515.jpg, 516.jpg ....]\n",
    "    |______ sunflowers: [560.jpg, 561.jpg, 562.jpg .....]\n",
    "    |______ tulips: [640.jpg, 641.jpg, 642.jpg ....]\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roses: 641 Images\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "Destination path '/home/shujat/.keras/datasets/flower_photos/train/roses/8036594516_69a7da5f73_m.jpg' already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-267d70de24f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.5/shutil.py\u001b[0m in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0mreal_dst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Destination path '%s' already exists\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: Destination path '/home/shujat/.keras/datasets/flower_photos/train/roses/8036594516_69a7da5f73_m.jpg' already exists"
     ]
    }
   ],
   "source": [
    "for cl in classes:\n",
    "  img_path = os.path.join(base_dir, cl)\n",
    "  images = glob.glob(img_path + '/*.jpg')\n",
    "  print(\"{}: {} Images\".format(cl, len(images)))\n",
    "  num_train = int(round(len(images)*0.8))\n",
    "  train, val = images[:num_train], images[num_train:]\n",
    "\n",
    "  for t in train:\n",
    "    if not os.path.exists(os.path.join(base_dir, 'train', cl)):\n",
    "      os.makedirs(os.path.join(base_dir, 'train', cl))\n",
    "    shutil.move(t, os.path.join(base_dir, 'train', cl))\n",
    "\n",
    "  for v in val:\n",
    "    if not os.path.exists(os.path.join(base_dir, 'val', cl)):\n",
    "      os.makedirs(os.path.join(base_dir, 'val', cl))\n",
    "    shutil.move(v, os.path.join(base_dir, 'val', cl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "513"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(len(images)*0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For convenience, let us set up the path for the training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(base_dir, 'train')\n",
    "val_dir = os.path.join(base_dir, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Set Batch and Image Size\\nApply Random Horizontal Flip\\nIn the cell below, use ImageDataGenerator to create a transformation that rescales the images by 255 and then applies a random horizontal flip. Then use the .flow_from_directory method to apply the above transformation to the images in our training set. Make sure you indicate the batch size, the path to the directory of the training images, the target size for the images, and to shuffle the images.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Set Batch and Image Size\n",
    "Apply Random Horizontal Flip\n",
    "In the cell below, use ImageDataGenerator to create a transformation that rescales the images by 255 and then applies a random horizontal flip. Then use the .flow_from_directory method to apply the above transformation to the images in our training set. Make sure you indicate the batch size, the path to the directory of the training images, the target size for the images, and to shuffle the images.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This function will plot images in the form of a grid with 1 row and 5 columns where images are placed in each column.\n",
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        ax.imshow(img)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "augmented_images = [train_data_gen[0][0][0] for i in range(5)]\n",
    "plotImages(augmented_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "IMG_SHAPE = 150 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2935 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "image_gen = ImageDataGenerator(rescale=1./255, horizontal_flip=True)\n",
    "\n",
    "train_data_gen = image_gen.flow_from_directory(\n",
    "                                                batch_size=batch_size, \n",
    "                                                directory=train_dir, \n",
    "                                                shuffle=True, \n",
    "                                                target_size=(IMG_SHAPE,IMG_SHAPE)\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
